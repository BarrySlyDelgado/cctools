#!/bin/sh

SCRIPT_NAME="$0"
CONFIG_FILE=$1

UUID="$(uuidgen)"

ROLE_NAME="makeflow-role-$UUID"
BUCKET_NAME="makeflow-bucket-$UUID"
FUNCTION_NAME="makeflow-function-$UUID"
REGION_NAME="us-east-1"
PROFILE_NAME="default"

FUNCTION_SOURCE="$FUNCTION_NAME".py
FUNCTION_PACKAGE="$FUNCTION_NAME".zip
ROLE_POLICY_FILE="makeflow-policy-$UUID.json"

if [ $# -ne 1 ]
then
	echo "use: $0 <config-file>"
	exit 1
fi

echo -n "Checking for aws command in PATH..."
if which aws >/dev/null 2>&1
then
	echo "ok"
else
	echo "failed"
	echo "$0: The \"aws\" command must be in your path to use this script."
	exit 1
fi

echo -n "Checking for aws configuration..."
if [ -f ~/.aws/config ]
then
	echo "ok"
else
	echo "failed"
	echo "$0 You must run \"aws configure\" before using this script."
	exit 1

fi

echo -n "Checking that aws command works..."
if aws ec2 describe-instances > /dev/null 2>&1
then
	echo "ok"
else
	echo "failed"
	echo "$0: Your Amazon credentials are not set up correctly. Try \"aws ec2 describe-instances\" to troubleshoot."
	exit 1
fi

# Create the role that will be attributed to the Lambda function. This
# gives the Lambda function 'permission' to access S3 and to execute

echo -n "Creating role policy file $ROLE_POLICY_FILE..."

# The skeleton of the role, to which we will attach the policies
cat > "$ROLE_POLICY_FILE" <<- EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "lambda.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
echo "ok"

echo -n "Creating iam role $ROLE_NAME..."
ROLE_ARN=$(aws iam create-role --output text --role-name "$ROLE_NAME" --assume-role-policy-document file://`pwd`/$ROLE_POLICY_FILE | grep "$ROLE_NAME" | awk '{print $2}')
if [ $? = 0 ]
then
	echo "ok"
else
	echo "failed!"
	exit 1
fi

rm "$ROLE_POLICY_FILE"

# This policy, when attached to the role, gives 'permission' to access S3
S3_ROLE_POLICY_ARN=arn:aws:iam::aws:policy/AmazonS3FullAccess

# This policy, when attached to the role, makes the Lambda function 'recognizable' by the Lambda service
LAMBDA_ROLE_POLICY_ARN=arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

echo -n "Attaching S3 policy to role $ROLE_ARN..."
aws iam attach-role-policy --role-name "$ROLE_NAME" --policy-arn "$S3_ROLE_POLICY_ARN"
if [ $? = 0 ]
then
	echo "ok"
else
	echo "failed!"
	exit 1
fi

echo -n "Attaching lambda execution policy to role $ROLE_ARN..."
aws iam attach-role-policy --role-name "$ROLE_NAME" --policy-arn "$LAMBDA_ROLE_POLICY_ARN"
if [ $? = 0 ]
then
	echo "ok"
else
	echo "failed!"
	exit 1
fi

# This is the Lambda function. 'event' is a JSON object passed to the function by the invoker.
cat > "$FUNCTION_SOURCE" <<- EOF
import boto3
import subprocess
import os
import shutil

def handler(event, context):
    s3 = boto3.client(service_name='s3', region_name=event["region_name"], config=boto3.session.Config(signature_version='s3v4'))
    # get inputs from S3 into filesystem
    bucket_name = event["bucket_name"]
    bucket_folder = event["bucket_folder"]
    work_dir = os.path.join("/tmp/", bucket_folder)
    os.mkdir(work_dir)
    for input_name in event["input_names"]:
        input_key = os.path.join(bucket_folder, input_name)
        input_path = os.path.join(work_dir, input_name)
        try:
            input_response = s3.get_object(Bucket=bucket_name, Key=input_key)
            try:
                os.makedirs(os.path.dirname(input_path))
            except OSError, e:
                if e.errno != 17:
                    raise
                pass
            input_file = open(input_path, "wb")
            input_file.write(input_response["Body"].read())
            input_file.close()
        except Exception as e:
            return "Error encountered while retrieving input\nbucket_name = {}\nbucket_folder = {}\nwork_dir = {}\ninput_name = {}\ninput_key = {}\ninput_path = {}\nevent = {}\nexception = {}\n".format(bucket_name, bucket_folder, work_dir, input_name, input_key, input_path, event, e)

    # run the command
    subprocess.call("cd "+work_dir+"; {}".format(event["cmd"]), shell=True)

    # put outputs into S3
    for output_name in event["output_names"]:
        output_key = os.path.join(bucket_folder, output_name)
        output_path = os.path.join(work_dir, output_name)
        try:
            output_file = open(output_path, "rb")
            output_response = s3.put_object(Bucket=bucket_name, Key=output_key, Body=output_file)
            output_file.close()
        except Exception as e:
            return "Error encountered while storing output\nbucket_name = {}\nbucket_folder = {}\nwork_dir = {}\noutput_name = {}\noutput_key = {}\noutput_path = {}\nevent = {}\nexception = {}\n".format(bucket_name, bucket_folder, work_dir, output_name, output_key, output_path, event, e)

    shutil.rmtree(work_dir)

    return "Lambda invocation successful\nevent = {}\n".format(event)
EOF

# This packages the code for deployment
zip -q "$FUNCTION_PACKAGE" "$FUNCTION_SOURCE"

echo -n "Creating lambda function $FUNCTION_NAME..."

while [ 1 ]
do
aws lambda create-function \
                 --function-name "$FUNCTION_NAME" \
                 --zip-file fileb://"$FUNCTION_PACKAGE" \
                 --role "$ROLE_ARN" \
                 --handler "$FUNCTION_NAME".handler \
                 --runtime python2.7 \
                 --timeout 300
if [ $? = 0 ]
then
	break
else
	echo -n "."
	sleep  1
fi

done

if [ $? = 0 ]
then
	echo "ok"
else
	echo "Failed!"
	exit 1
fi

rm "$FUNCTION_SOURCE"
rm "$FUNCTION_PACKAGE"

echo -n "Creating bucket $BUCKET_NAME..."
aws s3 mb s3://"$BUCKET_NAME" > /dev/null
if [ $? = 0 ]
then
	echo "ok"
else
	echo "failed"
	exit 1
fi

echo -n "Creating $CONFIG_FILE..."

printf '{"%s":"%s","%s":"%s","%s":"%s","%s":"%s","%s":"%s","%s":"%s","%s":"%s"}\n' "role_name" "$ROLE_NAME" "bucket_name" "$BUCKET_NAME" "function_name" "$FUNCTION_NAME" "region_name" "$REGION_NAME" "profile_name" "$PROFILE_NAME" "s3_role_policy_arn" "$S3_ROLE_POLICY_ARN" "lambda_role_policy_arn" "$LAMBDA_ROLE_POLICY_ARN" > "$CONFIG_FILE"

echo "ok"

exit 0


